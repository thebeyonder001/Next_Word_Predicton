# Next-Word-Prediction
DATA:
Dataset used is a part of Sherlock Dataset(https://www.gutenberg.org/files/1661/1661-0.txtc)
To use the data either upload the given text file "sherlock.txt" to your drive in the colab notebooks folder or change the
directory in the .ipynb file.
You can use some other text file too.

WORD EMBEDDING:
Word Embedding used: *Glove.6b.100d.txt* by stanford(https://nlp.stanford.edu/projects/glove/)
And to use it either upload the embedding in the colab notebooks folder in your drive or chamnge the directory in the .ipynb file

*MODEL Spec's*
* Input sequence length: 10 words
* Output: 1 word
* Used Self Attention Layer for better results
* Accuracy: 70% (which could be further improved by more training)
* Link to the trained Model(https://drive.google.com/file/d/1p_8AAZUnOn7ULQddTS7Hg1IZ7Lv_9uDU/view?usp=sharing)

![Model Summary](https://github.com/thebeyonder001/Next_Word_Predicton/blob/master/Model_Summary.jpeg)
